apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: trading-slo-rules
  labels:
    app: aether-platform
  annotations:
    slo.aether.dev/reference: docs/slo.md
spec:
  groups:
    - name: oms.slo
      rules:
        - alert: oms_latency_slo_breach
          expr: |
            histogram_quantile(0.99, sum(rate(oms_order_latency_seconds_bucket[5m])) by (le)) > 0.12
          for: 10m
          labels:
            severity: critical
            slo_target: "p99<=0.150s"
          annotations:
            summary: OMS latency approaching 150 ms SLO budget
            runbook: docs/runbooks/exchange_outage.md
            slo_reference: docs/slo.md#oms-latency
    - name: websocket.slo
      rules:
        - alert: ws_latency_slo_breach
          expr: |
            histogram_quantile(0.99, sum(rate(ws_delivery_latency_seconds_bucket[5m])) by (le)) > 0.25
          for: 15m
          labels:
            severity: warning
            slo_target: "p99<=0.300s"
          annotations:
            summary: WebSocket delivery latency nearing 300 ms SLO
            runbook: docs/runbooks/websocket_desync.md
            slo_reference: docs/slo.md#websocket-ingest-latency
    - name: risk-controls.slo
      rules:
        - alert: kill_switch_slo_warning
          expr: |
            histogram_quantile(
              0.99,
              sum(rate(kill_switch_response_seconds_bucket[5m])) by (le)
            ) > 45
          for: 1m
          labels:
            severity: critical
            slo_target: "response<=60s"
          annotations:
            summary: Kill-switch response time exceeded 45s threshold
            runbook: docs/runbooks/kill_switch_activation.md
            slo_reference: docs/slo.md#kill-switch-response
    - name: model-canary.slo
      rules:
        - alert: model_canary_promotion_slow
          expr: |
            (sum(increase(model_canary_promotion_duration_minutes_count[24h])) > 0)
            and ignoring(job)
            (sum(increase(model_canary_promotion_duration_minutes_bucket{le="30"}[24h]))
             < sum(increase(model_canary_promotion_duration_minutes_count[24h])))
          for: 5m
          labels:
            severity: warning
            slo_target: "p95<=45m"
          annotations:
            summary: Consecutive canary promotions exceeding 30 minute alert threshold
            runbook: docs/runbooks/model_rollback.md
            slo_reference: docs/slo.md#model-canary-promotion
    - name: infra.scaling
      rules:
        - alert: scaling_controller_evaluations_stalled
          expr: increase(scaling_evaluations_total[15m]) < 1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: Scaling controller has not completed an evaluation in the last 15 minutes
            runbook: docs/runbooks/scaling_controller.md
        - alert: scaling_gpu_pool_idle
          expr: |
            max_over_time(scaling_gpu_nodes[30m]) > 0
            and max_over_time(scaling_pending_training_jobs[30m]) < 1
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: GPU node pool provisioned without pending jobs for 30 minutes
            runbook: docs/runbooks/scaling_controller.md
    - name: trade-latency.slo
      rules:
        - alert: policy_latency_p95_slo_breach
          expr: |
            histogram_quantile(0.95,
              sum(rate(policy_latency_ms_bucket[5m])) by (le, symbol_tier, account_segment)
            ) > 200
          for: 10m
          labels:
            severity: warning
            slo_target: "p95<=200ms"
          annotations:
            summary: Policy evaluation latency p95 above 200 ms for {{ $labels.symbol_tier }}/{{ $labels.account_segment }}
            runbook: docs/runbooks/policy_latency.md
            slo_reference: docs/slo.md#policy-latency
        - alert: risk_latency_p95_slo_breach
          expr: |
            histogram_quantile(0.95,
              sum(rate(risk_latency_ms_bucket[5m])) by (le, symbol_tier, account_segment)
            ) > 200
          for: 10m
          labels:
            severity: warning
            slo_target: "p95<=200ms"
          annotations:
            summary: Risk validation latency p95 above 200 ms for {{ $labels.symbol_tier }}/{{ $labels.account_segment }}
            runbook: docs/runbooks/risk_latency.md
            slo_reference: docs/slo.md#risk-latency
        - alert: oms_latency_p95_slo_breach
          expr: |
            histogram_quantile(0.95,
              sum(rate(oms_latency_ms_bucket[5m])) by (le, symbol_tier, account_segment)
            ) > 500
          for: 10m
          labels:
            severity: critical
            slo_target: "p95<=500ms"
          annotations:
            summary: OMS submission latency p95 above 500 ms for {{ $labels.symbol_tier }}/{{ $labels.account_segment }}
            runbook: docs/runbooks/oms_latency.md
            slo_reference: docs/slo.md#oms-latency
    - name: infra.clock
      rules:
        - alert: clock_drift_exceeds_200ms
          expr: |
            max_over_time(abs(chrony_tracking_last_offset_seconds)[5m]) > 0.2
          for: 10m
          labels:
            severity: critical
          annotations:
            summary: Clock drift above 200 ms detected on {{ $labels.instance }}
            description: >-
              The Chrony exporter reported wall-clock drift greater than 200 ms for the
              last 10 minutes. Trading systems rely on sub-200 ms synchronisation to
              validate exchange timestamps and regulatory audit trails.
            runbook: docs/runbooks/clock_drift.md
